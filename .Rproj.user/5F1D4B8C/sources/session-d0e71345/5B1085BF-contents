---
title: "HW1 — SCF (2022)"
author: "Your Name"
date: "2025-11-08"
output:
  pdf_document:
    toc: true
    number_sections: true
fontsize: 11pt
---



# Notes

This RMarkdown solves Homework 1 (SCF-1) using the scripts and workflow in this folder. It will: (1) run the existing solution scripts if present, (2) load the generated CSVs/PNGs and display the canonical tables and figures, and (3) include concise written answers. If you prefer a fully self-contained run, enable the data-download chunks below and run in RStudio.

---

## 0 — Run the solution scripts (if present)

If you have the helper scripts in this folder (created earlier), this chunk will run them to (re)generate outputs. If you prefer not to run them automatically, set run_scripts <- FALSE.


``` r
run_scripts <- TRUE
if (run_scripts){
  scripts <- c('hw1_solutions.R','hw1_group_plot_other.R','hw1_group_plot_ineq.R')
  for (s in scripts){
    if (file.exists(file.path(base_dir, s))){
      message('Sourcing: ', s)
      tryCatch(source(file.path(base_dir, s)), error = function(e) message('Failed to run ', s, ': ', e$message))
    } else {
      message('Script not found (skipping): ', s)
    }
  }
} else message('run_scripts is FALSE — not executing external scripts')
```

```
## Downloading SCF summary and replicate weights (this may take a moment)...
## Data downloaded. Preparing variables...
## 
## PART (i) Unweighted mean and classic SE
## Unweighted: n = 22975, mean = 1.99564e+07, se = 726839
## 
## PART (ii) Weighted mean and classic weighted SE
## Weighted (naive): mean = 1.05946e+06, se = 15011.7
## 
## PART (iii) Rubin's rule across implicates (use sampling weights)
## Implicates M = 5
## Theta per implicate: 19437695.688, 19629604.64, 19444304.744, 19276498.903, 19947589.226
## Rubin: theta_bar = 1.95471e+07, SE = 932432
## 
## PART (iv) Replicate weights (only for first implicate)
## Detected 999 replicate columns (using first 999)
## Implicate 1: theta = 1.04182e+06, replicate-based SE = 78594.8 (R=998 used)
## 
## PART (v) Combine implicates + replicate weights
## Combined result: theta_bar = 1.05946e+06
## Within (replicate) variance W = 9.23347e+09
## Between variance B = 1.61989e+08
## Total variance T = 9.42786e+09, SE = 97097.2
## 
## Done. Results summary:
## # A tibble: 5 x 3
##   method                  estimate      se
##   <chr>                      <dbl>   <dbl>
## 1 Unweighted (i)         19956403. 726839.
## 2 Weighted naive (ii)     1059457.  15012.
## 3 Rubin sampling (iii)   19547139. 932432.
## 4 Replicates imp1 (iv)    1041815.  78595.
## 5 Rubin + replicates (v)  1059457.  97097.
## 
## PART (c) Using survey + mitools to compute mean and SE
## survey+mitools point estimate and SE:
## Estimate = 1.05946e+06, SE = 46959.6
## # A tibble: 6 x 3
##   method                  estimate      se
##   <chr>                      <dbl>   <dbl>
## 1 Unweighted (i)         19956403. 726839.
## 2 Weighted naive (ii)     1059457.  15012.
## 3 Rubin sampling (iii)   19547139. 932432.
## 4 Replicates imp1 (iv)    1041815.  78595.
## 5 Rubin + replicates (v)  1059457.  97097.
## 6 survey+mitools (c)      1059457.  46960.
## 
## Problem 2(a): mean(networth) and SE by `hhsex` and `racecl5` plus weighted counts
## 
## Mean(networth) by hhsex (survey+mitools):
##         1         2 
## 1330436.5  358141.5 
##       1       2 
## 61388.5 35124.9 
## 
## Weighted population counts (by hhsex):
##        1        2 
## 94709757 36596633 
## 
## Mean(networth) by racecl5 (survey+mitools):
##         1         2         3         4         5 
## 1361785.1  211511.7  227523.4 1810102.3  389473.8 
##         1         2         3         4         5 
##  63203.01  39885.20  26517.11 319343.65  51448.57 
## 
## Weighted population counts (by racecl5):
##        1        2        3        4        5 
## 87725120 15118565 12284804  5181424 10996477 
## 
## Saved summary results to c:/Users/takao/Documents/wu-seminar-distribution/hw1/hw1_results.csv
## 
## Computing SE for different numbers of replicate weights (this may take a moment)...
##   - building design with 10 replicate weights...
##     -> SE = 44329.3
##   - building design with 50 replicate weights...
##     -> SE = 40748.6
##   - building design with 100 replicate weights...
##     -> SE = 40036.2
##   - building design with 250 replicate weights...
##     -> SE = 46049.3
##   - building design with 500 replicate weights...
##     -> SE = 46372
##   - building design with 999 replicate weights...
##     -> SE = 46959.6
## Saved SE vs replicates data to c:/Users/takao/Documents/wu-seminar-distribution/hw1/se_vs_reps.csv
```

```
## Saved SE stability plot to c:/Users/takao/Documents/wu-seminar-distribution/hw1/se_vs_reps.png
```

```
## Downloading SCF summary and replicate weights...
## Selected grouping variable: own
## Building svrepdesign (this may take a moment)...
## Saved group results to c:/Users/takao/Documents/wu-seminar-distribution/hw1/group_by_own_networth.csv
```

```
## Saved bar chart to c:/Users/takao/Documents/wu-seminar-distribution/hw1/group_by_own_networth.png
## 
## Done.
```

```
## Downloading SCF summary and replicate weights...
## Shifting networth by 555501 to compute log(networth + shift)
## Building svrepdesign...
## Saved group log-networth results to c:/Users/takao/Documents/wu-seminar-distribution/hw1/group_by_own_log_networth.csv
```

```
## Saved inequality plot to c:/Users/takao/Documents/wu-seminar-distribution/hw1/group_by_own_log_networth.png
## Done.
```

---

## 1. Weights, Implicates, Replicate weights (Question 1)

Below I present the answers for parts 1(a)(i–v), 1(b) conceptual notes, and 1(c) the canonical survey+mitools implementation. The essential code is included and the canonical results (if produced by the scripts) are displayed.

### 1(a) Step-by-step computations (essential code)

The chunk below contains compact implementations of the steps. It assumes a data.frame `scf` with columns: networth, wgt, imp and (optionally) replicate weights.


``` r
# Example minimal steps — enable when scf is available (or run after running hw1_solutions.R)
# (i) Unweighted mean
y <- scf$networth
n <- sum(!is.na(y))
mean_unw <- mean(y, na.rm=TRUE)
se_unw <- sd(y, na.rm=TRUE)/sqrt(n)

# (ii) Weighted naive
w <- scf$wgt
mean_w <- sum(w * scf$networth, na.rm = TRUE) / sum(w, na.rm = TRUE)
var_w_mean <- sum((w^2) * (scf$networth - mean_w)^2, na.rm = TRUE) / (sum(w, na.rm = TRUE)^2)
se_w_naive <- sqrt(var_w_mean)

# (iii) Rubin's rules across implicates (sampling-weight within-variance approx)
M <- length(unique(scf$imp))
imp_stats <- scf %>% group_by(imp) %>% summarize(
  theta = sum(w * networth, na.rm = TRUE) / sum(w, na.rm = TRUE),
  W = sum((w^2) * (networth - theta)^2, na.rm=TRUE) / (sum(w, na.rm=TRUE)^2), .groups='drop')

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = FALSE,
											fig.width = 7, fig.height = 4)
options(width = 120)

# Load commonly-used packages; heavy packages used only in eval=TRUE chunks
library(knitr)
library(readr)
library(dplyr)
library(ggplot2)
```

# Introduction

This document contains the homework questions followed immediately by concise, self-contained answers and the essential R code needed to reproduce the results. Heavy download or long-running chunks are disabled (eval=FALSE); run them interactively in RStudio to reproduce the full analysis. Pre-generated tables and figures that already exist in the folder will be embedded automatically (so your figures remain in the PDF).


# (iii) Rubin across implicates using sampling-weight within variances

Statement: Compute mean household net worth (variable: networth) and its standard error under the following steps. Explain differences in point estimates and uncertainty between steps.

1. Unweighted mean and classic SE (treat sample as SRS)
2. Weighted mean with a naive SE (uses sampling weights but ignores design)
3. Combine implicates via Rubin's rules (use sampling-weight within variances)
4. Compute replicate-based SE for one implicate (use replicate weights)
5. Combine implicates using replicate-based within-variances (design + imputation aware)

### Answer — essential code and short explanations

Below are the compact, runnable code snippets that implement the essential computations. All heavy I/O is in the first chunk (disabled by default).

# compute per-implicate weighted mean and within-variance
# Download SCF public summary and replicate-weight files and read the .dta
# Run this interactively in RStudio (eval=TRUE) when you want to reproduce end-to-end
library(haven); library(dplyr)
summary_url <- 'https://www.federalreserve.gov/econres/scf/files/2022/scfp2022s.zip'
rw_url <- 'https://www.federalreserve.gov/econres/scf/files/2022/scf2022rw1s.zip'

tmp <- tempdir()
d1 <- file.path(tmp, basename(summary_url))
d2 <- file.path(tmp, basename(rw_url))
download.file(summary_url, d1, mode = 'wb')
download.file(rw_url, d2, mode = 'wb')
unzip(d1, exdir = tmp)
unzip(d2, exdir = tmp)

# read first .dta file found (adjust if necessary)
dtas <- list.files(tmp, pattern = '\\.(dta|DTA)$', full.names = TRUE)
scf_all <- read_dta(dtas[1])

# Minimal standardize: adapt column names if your version uses different names
# Expected: networth, wgt (main weight), imp (implicate id), plus replicate weight columns
scf <- scf_all %>% transmute(
	networth = as.numeric(networth),
	wgt = as.numeric(wgt),
	imp = as.integer(imp)
)
```

Explanation: The chunk above downloads and reads the SCF files and creates a minimal `scf` data frame. It is intentionally marked `eval=FALSE` to avoid long downloads during a quick knit. Run it in RStudio to produce `scf`.

M <- length(unique(scf$imp))
# (1) Unweighted mean and classic SE
y <- scf$networth
n <- sum(!is.na(y))
mean_unw <- mean(y, na.rm = TRUE)
se_unw <- sd(y, na.rm = TRUE) / sqrt(n)
list(mean = mean_unw, se = se_unw, n = n)
```

imp_means <- scf %>% group_by(imp) %>%
# (2) Weighted mean with naive SE (approximate)
w <- scf$wgt
mean_w <- sum(w * scf$networth, na.rm = TRUE) / sum(w, na.rm = TRUE)
# naive approximate variance for weighted mean
var_w_mean <- sum((w^2) * (scf$networth - mean_w)^2, na.rm = TRUE) / (sum(w, na.rm = TRUE)^2)
se_w_naive <- sqrt(var_w_mean)
list(weighted_mean = mean_w, se_naive = se_w_naive)
```

	summarize(theta = sum(w * networth, na.rm = TRUE) / sum(w, na.rm = TRUE),
# (3) Rubin across implicates (sampling-weight within variance)
M <- length(unique(scf$imp))
imp_stats <- scf %>% group_by(imp) %>%
	summarize(
		theta = sum(w * networth, na.rm = TRUE) / sum(w, na.rm = TRUE),
		W = sum((w^2) * (networth - theta)^2, na.rm = TRUE) / (sum(w, na.rm = TRUE)^2),
		.groups = 'drop'
	)

theta_bar <- mean(imp_stats$theta)
W_bar <- mean(imp_stats$W)
B <- var(imp_stats$theta)
T_var <- W_bar + (1 + 1/M) * B
se_rubin <- sqrt(T_var)
list(theta_bar = theta_bar, se = se_rubin)
```

						# within variance estimate (using weighted variance approx)
# (4) Replicate-based SE for implicate 1 (skeleton)
# You need a matrix/data.frame `repw` of replicate weights (columns) matching rows of imp1
imp1 <- scf %>% filter(imp == 1)
# repw <- <matrix with same nrow as imp1 with replicate weight columns>
# theta_hat <- sum(imp1$wgt * imp1$networth) / sum(imp1$wgt)
# theta_reps <- apply(repw, 2, function(wr) sum(wr * imp1$networth) / sum(wr))
# replicate variance depends on method (BRR, Fay, JK) and a scale factor
```

						W = sum((w^2) * (networth - theta)^2, na.rm = TRUE) / (sum(w, na.rm = TRUE)^2),
# (5) Combine implicates + replicate-based within variances
# For each implicate m compute theta_m and W_m (via replicate weights)
# Then W_bar_rep <- mean(W_m)
# B <- var(theta_m)
# Total variance T = W_bar_rep + (1 + 1/M) * B
```

						.groups = 'drop')

The document will embed your existing CSV tables and PNG figures if they are present in the working directory. This preserves the exact tables and images you already produced during prior runs.


library(knitr)
if (file.exists('hw1_results.csv')) print(kable(read.csv('hw1_results.csv'), digits = 2))
if (file.exists('racecl5_networth_by_group.csv')) print(kable(read.csv('racecl5_networth_by_group.csv'), digits = 2))
if (file.exists('hhsex_networth_by_group.csv')) print(kable(read.csv('hhsex_networth_by_group.csv'), digits = 2))
if (file.exists('group_by_own_networth.csv')) print(kable(read.csv('group_by_own_networth.csv'), digits = 2))
if (file.exists('group_by_own_networth.png')) print(knitr::include_graphics('group_by_own_networth.png'))
if (file.exists('group_by_own_log_networth.csv')) print(kable(read.csv('group_by_own_log_networth.csv'), digits = 4))
if (file.exists('group_by_own_log_networth.png')) print(knitr::include_graphics('group_by_own_log_networth.png'))
```


W_bar <- mean(imp_means$W)

- Weights: produce population-representative estimates by correcting for unequal selection probabilities and differential nonresponse.
- Implicates: reflect uncertainty from imputed values; Rubin's rules propagate that uncertainty into total variance.
- Replicate weights: capture sampling design (strata, clustering) and provide design-consistent variance estimates; they typically increase SEs relative to naive formulas.

A canonical approach is to build a replicate-weighted survey design for each implicate and then combine estimates with `mitools::MIcombine()` — an essential code sketch is below.

B <- var(imp_means$theta)
# canonical: svrepdesign per implicate + MIcombine
library(survey); library(mitools)
# Suppose imp_list is a list of data.frames (one per implicate) with columns: networth, wgt, repw1..repwR
designs <- lapply(imp_list, function(df) {
	svrepdesign(weights = ~wgt, repweights = df[, grep('^repw', names(df))], data = df,
							type = 'other', combined.weights = TRUE)
})
res_list <- lapply(designs, function(d) with(d, svymean(~networth)))
res_mi <- MIcombine(res_list)
summary(res_mi)
```


se_rubin <- sqrt(T_var)

1. Compute mean networth and SE by `hhsex` and by `racecl5` (separately).
2. Pick another characteristic (we use `own` = homeownership) and show mean networth + 95% CI in a bar chart.
3. Pick an inequality indicator (we use log(networth + shift)) and compare by `own`.

list(theta_bar = theta_bar, se_rubin = se_rubin)

```
library(dplyr); library(ggplot2)
# group-weighted mean with naive SE (illustrative)
group_weighted_mean <- function(df, grp){
	df %>% group_by(!!sym(grp)) %>%
		summarize(
			weighted_mean = sum(wgt * networth, na.rm=TRUE) / sum(wgt, na.rm=TRUE),
			se = sqrt(sum((wgt^2) * (networth - weighted_mean)^2, na.rm=TRUE) / (sum(wgt, na.rm=TRUE)^2)),
			n = n(), .groups = 'drop'
		)
}

# Example: by hhsex and race
by_hhsex <- group_weighted_mean(scf, 'hhsex')
by_race  <- group_weighted_mean(scf, 'racecl5')
```


# Bar chart for 'own'
own_tab <- group_weighted_mean(scf, 'own')
own_tab <- own_tab %>% mutate(lower = weighted_mean - 1.96 * se, upper = weighted_mean + 1.96 * se)
ggplot(own_tab, aes(x = factor(own), y = weighted_mean)) +
	geom_col(width = 0.6) +
	geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.15) +
	labs(x = 'Own (0=No,1=Yes)', y = 'Weighted mean networth', title = 'Mean networth by homeownership')
```


``` r
# Inequality: mean log(networth + shift)
shift <- abs(min(scf$networth, na.rm=TRUE)) + 1
scf <- scf %>% mutate(log_net = log(networth + shift))
own_log_tab <- group_weighted_mean(scf %>% mutate(networth = log_net), 'own')
own_log_tab <- own_log_tab %>% mutate(lower = weighted_mean - 1.96 * se, upper = weighted_mean + 1.96 * se)

ggplot(own_log_tab, aes(x = factor(own), y = weighted_mean)) +
	geom_col(width = 0.6) + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.15) +
	labs(x = 'Own', y = 'Mean log(networth+shift)', title = 'Mean log(networth+shift) by homeownership')
```

# (iv) Replicate weights for the first implicate: compute replicate estimates

The document will embed your existing CSV tables and PNG figures if they are present in the working directory. This preserves the exact tables and images you already produced during prior runs.

# Assume you have a matrix/data.frame `repweights` with R columns matching household rows.
library(knitr)
if (file.exists('hw1_results.csv')) print(kable(read.csv('hw1_results.csv'), digits = 2))
if (file.exists('racecl5_networth_by_group.csv')) print(kable(read.csv('racecl5_networth_by_group.csv'), digits = 2))
if (file.exists('hhsex_networth_by_group.csv')) print(kable(read.csv('hhsex_networth_by_group.csv'), digits = 2))
if (file.exists('group_by_own_networth.csv')) print(kable(read.csv('group_by_own_networth.csv'), digits = 2))
if (file.exists('group_by_own_networth.png')) print(knitr::include_graphics('group_by_own_networth.png'))
if (file.exists('group_by_own_log_networth.csv')) print(kable(read.csv('group_by_own_log_networth.csv'), digits = 4))
if (file.exists('group_by_own_log_networth.png')) print(knitr::include_graphics('group_by_own_log_networth.png'))
```


imp1 <- filter(scf, imp == 1)

- Run the `download-data` chunk (eval=TRUE) in RStudio to obtain the public SCF files and build the `scf` data frame.
- Install the required R packages if missing: `haven`, `dplyr`, `survey`, `mitools`, `ggplot2`, `knitr`, `rmarkdown`, `tinytex`.
- For PDF output, ensure a working LaTeX installation (TinyTeX is recommended):

```r
install.packages('tinytex')
tinytex::install_tinytex()
```

- To render the document from the command line you may point `RSTUDIO_PANDOC` to your RStudio pandoc installation or install pandoc system-wide.


# theta_rep <- apply(repweights, 2, function(wrep) sum(wrep * imp1$networth) / sum(wrep))
# use appropriate replicate variance formula (depends on replication method, BRR, JK, etc.)
# e.g., for BRR: var_rep = sum((theta_rep - theta_hat)^2) * scale
```


``` r
# (v) Combine implicates + replicate-based within-variances
# For each implicate, compute the replicate-based within variance (W_m)
# Then apply Rubin: total variance = W_bar + (1+1/M)B where W_bar uses replicate-based W_m
# Pseudocode:
# W_ms <- c(W1_rep, W2_rep, ..., WM_rep)
# W_bar_rep <- mean(W_ms)
# B <- var(theta_m)
# T_var_combined <- W_bar_rep + (1 + 1/M) * B
# se_combined <- sqrt(T_var_combined)
```


```
## 
## 
## |method                 | estimate|        se|
## |:----------------------|--------:|---------:|
## |Unweighted (i)         | 19956403| 726839.32|
## |Weighted naive (ii)    |  1059457|  15011.67|
## |Rubin sampling (iii)   | 19547139| 932431.86|
## |Replicates imp1 (iv)   |  1041815|  78594.79|
## |Rubin + replicates (v) |  1059457|  97097.15|
## |survey+mitools (c)     |  1059457|  46959.57|
```

Short explanation: the code above shows the essential steps — compute a point estimate per implicate; obtain a within-imputation variance per implicate (either via sampling-weight analytic approx or via replicate weights); take the mean within variance W_bar and between-imputation variance B; combine via Rubin's formula to get a total variance.

---

## Question 1(b) — Conceptual: why weights, implicates, replicate weights?

Answer:

- Weights: adjust for unequal selection probabilities so the estimator targets the population mean rather than the sample mean.
- Implicates: multiple imputations encode uncertainty about imputed values; combining across implicates inflates SEs appropriately when imputation uncertainty is large.
- Replicate weights: capture the sample design (strata/clustering) and provide design-consistent variance estimates; they often increase SEs relative to naive formulas.

An essential code snippet to demonstrate how survey+mitools implements the canonical estimator (illustrative; enable download chunk above first):


``` r
# canonical workflow: build svrepdesign for each implicate and combine with MIcombine
library(survey); library(mitools)
# pretend 'imp_list' is a list of data.frames (one per implicate) with columns networth and appropriate weights and replicate weights
# build svrepdesign for each implicate (example)
designs <- lapply(imp_list, function(df) svrepdesign(weights = ~wgt, repweights = df[, grep('^repw', names(df))], type='other', combined.weights=TRUE))
# use with() + MIcombine
res_list <- lapply(designs, function(d) with(d, svymean(~networth)))
res_mi <- MIcombine(res_list)
summary(res_mi)
```

---

## Question 2 — Application to population groups

Question (summary): compute mean household net worth and SE by `hhsex` and by `racecl5`; then pick another characteristic (we use `own` = homeownership) and show mean networth + 95% CI as a bar chart; finally pick an inequality-related variable (we use log(networth + shift)) and compare by the characteristic.

Answer — essential code and visualization (self-contained skeleton):


``` r
# assumes `scf` has been prepared as above and contains variables: networth, wgt, hhsex, racecl5, own
library(dplyr); library(ggplot2); library(survey); library(mitools)

# simple group-level weighted mean with naive SE
group_weighted_mean <- function(df, group_var){
	df %>% group_by(!!sym(group_var)) %>%
		summarize(weighted_mean = sum(wgt * networth, na.rm=TRUE) / sum(wgt, na.rm=TRUE),
							se = sqrt(sum((wgt^2) * (networth - weighted_mean)^2, na.rm=TRUE) / (sum(wgt, na.rm=TRUE)^2)),
							n = n())
}

# Example: by hhsex
by_hhsex <- group_weighted_mean(scf, 'hhsex')
by_race  <- group_weighted_mean(scf, 'racecl5')

by_hhsex; by_race
```


``` r
# (b) Bar chart by `own` (homeownership) with 95% CI (naive approximation)
own_tab <- group_weighted_mean(scf, 'own')
own_tab <- own_tab %>% mutate(lower = weighted_mean - 1.96 * se, upper = weighted_mean + 1.96 * se)
ggplot(own_tab, aes(x = factor(own), y = weighted_mean)) +
	geom_col() +
	geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +
	labs(x = 'Own (0=No, 1=Yes)', y = 'Weighted mean networth', title = 'Mean networth by homeownership')
```


```
## 
## 
## | group|  estimate|       se|   lower95|   upper95|
## |-----:|---------:|--------:|---------:|---------:|
## |     0|  487107.1| 70743.68|  348449.4|  625764.7|
## |     1| 1150427.6| 52092.55| 1048326.2| 1252529.0|
```

```
## 
## [1] "group_by_own_networth.png"
## attr(,"class")
## [1] "knit_image_paths" "knit_asis"
```


``` r
# (c) Inequality indicator: mean log(networth + shift) by `own`
# choose shift so (networth + shift) > 0 for all obs
shift <- abs(min(scf$networth, na.rm=TRUE)) + 1
scf <- scf %>% mutate(log_net = log(networth + shift))
own_log_tab <- group_weighted_mean(scf %>% mutate(networth = log_net), 'own')
own_log_tab <- own_log_tab %>% mutate(lower = weighted_mean - 1.96 * se, upper = weighted_mean + 1.96 * se)
ggplot(own_log_tab, aes(x = factor(own), y = weighted_mean)) +
	geom_col() + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +
	labs(x='Own', y='Mean log(networth+shift)', title='Mean log networth by homeownership')
```


```
## 
## 
## | group| estimate|     se| lower95| upper95|
## |-----:|--------:|------:|-------:|-------:|
## |     0|  13.4902| 0.0272| 13.4369| 13.5435|
## |     1|  13.8244| 0.0177| 13.7897| 13.8591|
```

```
## 
## [1] "group_by_own_log_networth.png"
## attr(,"class")
## [1] "knit_image_paths" "knit_asis"
```

Short interpretation: the visual and numeric outputs show that owners have substantially higher mean net worth than non-owners and also higher mean log networth (less sensitive to extremes). For rigorous SEs use the canonical `svrepdesign` + `MIcombine` approach shown in Question 1(c).

---

## Notes on running this document

- The code chunks marked `eval=FALSE` are the essential reproducible steps; when you run this in RStudio enable or run them interactively (they download the public SCF summary and replicate weights and then reproduce the calculations). RStudio bundles pandoc so knitting to PDF or HTML is simpler there.
- If you prefer a fully non-interactive reproduce, change the `eval=FALSE` chunks to `eval=TRUE` and knit after installing `tinytex` (or ensuring your LaTeX/MiKTeX installation is functional).

If you want, I can now toggle some of the heavier chunks to `eval=TRUE` and attempt a local render here (note: rendering earlier failed due to missing pandoc/LaTeX in this environment). Alternatively I can run a quick diagnostic of your `hw1_submission.log` if you still see errors when knitting locally.


